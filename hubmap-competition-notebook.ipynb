{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install segmentation_models","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:08:56.922118Z","iopub.execute_input":"2023-07-20T10:08:56.922852Z","iopub.status.idle":"2023-07-20T10:09:08.708201Z","shell.execute_reply.started":"2023-07-20T10:08:56.922816Z","shell.execute_reply":"2023-07-20T10:09:08.706971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1, 2, 3'\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\nos.environ['TF_DISABLE_LAYOUT_OPTIMIZATION'] = '1'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimporting the necessary libraries\n\"\"\"\n\n\nimport os\nimport sys\nimport json\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport tensorflow as tf\nimport segmentation_models as sm\nimport albumentations as A\nimport numpy as np\nimport cv2\n\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom scipy.ndimage import binary_fill_holes\nfrom tensorflow.keras import layers\nfrom PIL import Image\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-20T10:07:06.994518Z","iopub.execute_input":"2023-07-20T10:07:06.995032Z","iopub.status.idle":"2023-07-20T10:07:07.003231Z","shell.execute_reply.started":"2023-07-20T10:07:06.994989Z","shell.execute_reply":"2023-07-20T10:07:07.002186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"physical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    tf.config.set_visible_devices(physical_devices[0], 'GPU')","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:07:07.005004Z","iopub.execute_input":"2023-07-20T10:07:07.005599Z","iopub.status.idle":"2023-07-20T10:07:07.019022Z","shell.execute_reply.started":"2023-07-20T10:07:07.005568Z","shell.execute_reply":"2023-07-20T10:07:07.01798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lr_schedule(epoch):\n    # You can define any learning rate schedule you want based on the epoch number\n    if epoch < 20:\n        lr = 0.0001\n    elif epoch > 20 and epoch < 60:\n        lr = 0.00001\n    else:\n        kr = 0.000001\n    return lr\n\nlr_scheduler = LearningRateScheduler(lr_schedule)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:07:07.0361Z","iopub.execute_input":"2023-07-20T10:07:07.037069Z","iopub.status.idle":"2023-07-20T10:07:07.048668Z","shell.execute_reply.started":"2023-07-20T10:07:07.037037Z","shell.execute_reply":"2023-07-20T10:07:07.047566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomLoss(tf.keras.losses.Loss):\n    def __init__(self, name='custom_loss', **kwargs):\n        super(CustomLoss, self).__init__(name=name, **kwargs)\n        self.diceloss = sm.losses.DiceLoss()\n        self.binloss = sm.losses.BinaryCELoss()\n\n    def call(self, y_true, y_pred):\n        dice = self.diceloss(y_true, y_pred)\n        bce = self.binloss(y_true, y_pred)\n        loss = dice * 0.7 + bce * 0.3\n        return loss\n","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:12:03.666309Z","iopub.execute_input":"2023-07-20T10:12:03.666755Z","iopub.status.idle":"2023-07-20T10:12:03.674077Z","shell.execute_reply.started":"2023-07-20T10:12:03.666714Z","shell.execute_reply":"2023-07-20T10:12:03.672896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_loss = CustomLoss()\ndice_metric = sm.metrics.FScore(smooth=1e-05)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:12:06.170337Z","iopub.execute_input":"2023-07-20T10:12:06.171032Z","iopub.status.idle":"2023-07-20T10:12:06.176137Z","shell.execute_reply.started":"2023-07-20T10:12:06.170992Z","shell.execute_reply":"2023-07-20T10:12:06.175075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BACKBONE = 'efficientnetb1'\npreprocess_input = sm.get_preprocessing(BACKBONE)\nmodel = sm.Unet(BACKBONE, classes=1, activation='sigmoid')\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\nmodel.compile(\n    optimizer = optimizer,\n    loss=custom_loss,\n    metrics=[sm.metrics.iou_score , dice_metric],\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:12:09.0491Z","iopub.execute_input":"2023-07-20T10:12:09.049462Z","iopub.status.idle":"2023-07-20T10:12:15.492615Z","shell.execute_reply.started":"2023-07-20T10:12:09.049432Z","shell.execute_reply":"2023-07-20T10:12:15.491674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nTHe below pieces of code is directly from one of the submitters who was kind enough to share\n\"\"\"\n\nclass Acquisition:\n    \n    def get_datframe(self,path):\n        return pd.read_csv(path)\n    \n    def get_json_dataframe(self, json_file):\n        data = []\n        with open(json_file, 'r') as file:\n            for line in file:\n                item = json.loads(line)\n                data.append(item)\n        \n        json_df = pd.DataFrame(data)\n        return json_df\n    \n        \nacq = Acquisition()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:12:33.309172Z","iopub.execute_input":"2023-07-20T10:12:33.309601Z","iopub.status.idle":"2023-07-20T10:12:33.320612Z","shell.execute_reply.started":"2023-07-20T10:12:33.309565Z","shell.execute_reply":"2023-07-20T10:12:33.319435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title=acq.get_datframe('/kaggle/input/hubmap-hacking-the-human-vasculature/tile_meta.csv')\ntitle.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:12:33.323025Z","iopub.execute_input":"2023-07-20T10:12:33.323819Z","iopub.status.idle":"2023-07-20T10:12:33.374365Z","shell.execute_reply.started":"2023-07-20T10:12:33.323783Z","shell.execute_reply":"2023-07-20T10:12:33.372563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wsi = acq.get_datframe(path='/kaggle/input/hubmap-hacking-the-human-vasculature/wsi_meta.csv')\nwsi.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:12:33.375743Z","iopub.execute_input":"2023-07-20T10:12:33.376255Z","iopub.status.idle":"2023-07-20T10:12:33.39978Z","shell.execute_reply.started":"2023-07-20T10:12:33.376216Z","shell.execute_reply":"2023-07-20T10:12:33.398663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"polygons_df = acq.get_json_dataframe('/kaggle/input/hubmap-hacking-the-human-vasculature/polygons.jsonl')\npolygons_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:12:33.402364Z","iopub.execute_input":"2023-07-20T10:12:33.402836Z","iopub.status.idle":"2023-07-20T10:12:38.20198Z","shell.execute_reply.started":"2023-07-20T10:12:33.4028Z","shell.execute_reply":"2023-07-20T10:12:38.200923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nWe are separating the annotations from the json file to the very atomic state.\nEach annotation is a new row in the created dataframe\n\"\"\"\n\ndef separate_annotations(dataset):\n    separated  = pd.DataFrame(columns=[\"id\",\"type\",\"coordinates\"])\n    for index in dataset.index:\n        id = dataset[\"id\"][index]\n        all_annotations = dataset[\"annotations\"][index]\n        for each_annotation in all_annotations:\n            annotation_type = each_annotation[\"type\"]\n            annotation_coordinates = each_annotation[\"coordinates\"]\n            separated.loc[len(separated)]=[id,annotation_type,annotation_coordinates]\n    return separated\n\nseparated_polygons_df  = separate_annotations(polygons_df)\nseparated_polygons_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:12:38.20355Z","iopub.execute_input":"2023-07-20T10:12:38.204656Z","iopub.status.idle":"2023-07-20T10:13:14.478Z","shell.execute_reply.started":"2023-07-20T10:12:38.204601Z","shell.execute_reply":"2023-07-20T10:13:14.476816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_augmentation(p=1.0):\n    return A.Compose([\n        A.HorizontalFlip(),\n        A.VerticalFlip(),\n        A.RandomRotate90(),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.60,\n                         border_mode=cv2.BORDER_REFLECT),\n        A.OneOf([\n            A.ElasticTransform(p=.3),\n            A.GaussianBlur(p=.3),\n            A.GaussNoise(p=.3),\n            A.OpticalDistortion(p=0.3),\n            A.GridDistortion(p=.1),\n        ], p=0.3),\n        A.OneOf([\n            A.HueSaturationValue(15,25,0),\n            A.CLAHE(clip_limit=2),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        ], p=0.3),\n    ], p=p)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:13:14.479658Z","iopub.execute_input":"2023-07-20T10:13:14.480138Z","iopub.status.idle":"2023-07-20T10:13:14.491346Z","shell.execute_reply.started":"2023-07-20T10:13:14.4801Z","shell.execute_reply":"2023-07-20T10:13:14.490332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(file_name, image_size=(512, 512), augmentation=None):\n    if 'tif' not in file_name:\n        path = '/kaggle/input/hubmap-hacking-the-human-vasculature/train/{}.tif'.format(file_name)\n    else:\n        path = '/kaggle/input/hubmap-hacking-the-human-vasculature/train/{}'.format(file_name)\n    image = cv2.imread(path)\n    #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    image = np.asarray(image, dtype=np.uint8)\n\n    mask = np.zeros(image_size, dtype=np.float32)\n    filter_criteria = separated_polygons_df[\"id\"] == file_name\n    all_coordinates = separated_polygons_df.loc[filter_criteria, \"coordinates\"].tolist()\n    all_type = separated_polygons_df.loc[filter_criteria, \"type\"].tolist()\n    for i in range(len(all_coordinates)):\n        if all_type[i] == \"blood_vessel\":\n            x_values = [point[0] for point in all_coordinates[i][0]]\n            y_values = [point[1] for point in all_coordinates[i][0]]\n            mask[x_values, y_values] = 1\n    mask = binary_fill_holes(mask)\n    mask = mask.astype(np.float32)\n\n    # Apply data augmentation if provided\n    if augmentation is not None:\n        augmented = augmentation(image=image, mask=mask)\n        image, mask = augmented[\"image\"], augmented[\"mask\"]\n\n    return image, np.expand_dims(mask, axis=-1)\n\n\nsample_image = separated_polygons_df[\"id\"][1]\nimage, mask = preprocess_image(sample_image, augmentation=get_augmentation(p=1.0))\nimage = image/255.0\nmask = mask.astype(np.uint8)\nprint(image.shape)\nprint(mask.shape)\n# Display the image and mask\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\nif image is not None or mask is not None:\n    axes[0].imshow(image)\n    axes[0].set_title(\"Image\")\n    axes[0].axis(\"off\")\n    axes[1].imshow(mask)\n    axes[1].set_title(\"Mask\")\n    axes[1].axis(\"off\")\n    plt.show()\n\nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:13:14.492663Z","iopub.execute_input":"2023-07-20T10:13:14.493367Z","iopub.status.idle":"2023-07-20T10:13:15.006884Z","shell.execute_reply.started":"2023-07-20T10:13:14.493327Z","shell.execute_reply":"2023-07-20T10:13:15.006002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataGen(tf.keras.utils.Sequence):\n    def __init__(self, df,batch_size, input_size=(512, 512, 3),shuffle=True ,test_size=0.1, val_size=0.1):\n        self.all_unique_images = df[\"id\"].unique()\n        self.batch_size = batch_size\n        self.input_size = input_size\n        self.shuffle = shuffle\n        \n        \n        train_val_indices, self.test_indices = train_test_split(\n            np.arange(len(self.all_unique_images)),\n            test_size=test_size,\n            shuffle=shuffle\n        )\n        \n        # Split the remaining train_val_indices into train and validation sets\n        self.train_indices, self.val_indices = train_test_split(\n            train_val_indices,\n            test_size=val_size,\n            shuffle=shuffle\n        )\n    def __len__(self):\n        return len(self.all_unique_images)//self.batch_size\n    \n    def __getitem__(self, index):\n        batch_indices = self.all_unique_images[index * self.batch_size:(index + 1) * self.batch_size]\n        images = []\n        masks = []\n        for each_image in batch_indices:\n            image, mask = preprocess_image(each_image,augmentation=get_augmentation(p=1.0))\n            images.append(image)\n            masks.append(mask)\n\n        images = np.asarray(images, dtype=np.float32) / 255.0\n        masks = np.asarray(masks, dtype=np.float32)\n        return images, masks\n    \n    def get_validation_data(self):\n        images = []\n        masks = []\n        for each_image in self.val_indices:\n            image, mask = preprocess_image(self.all_unique_images[each_image])\n            images.append(image)\n            masks.append(mask)\n\n        images = np.asarray(images, dtype=np.float32) / 255.0\n        masks = np.asarray(masks, dtype=np.float32)\n\n        return images, masks\n\n    def get_test_data(self):\n        images = []\n        masks = []\n        for each_image in self.test_indices:\n            image, mask = preprocess_image(self.all_unique_images[each_image])\n            images.append(image)\n            masks.append(mask)\n\n        images = np.asarray(images, dtype=np.float32) / 255.0\n        masks = np.asarray(masks, dtype=np.float32)\n\n        return images, masks\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:13:15.010222Z","iopub.execute_input":"2023-07-20T10:13:15.01057Z","iopub.status.idle":"2023-07-20T10:13:15.028512Z","shell.execute_reply.started":"2023-07-20T10:13:15.010537Z","shell.execute_reply":"2023-07-20T10:13:15.02775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_generator = CustomDataGen(separated_polygons_df, batch_size=8, test_size=0.1, val_size=0.15)\nval_images, val_masks = data_generator.get_validation_data()  # Accessing the validation data\ntest_images, test_masks = data_generator.get_test_data() ","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:13:15.030235Z","iopub.execute_input":"2023-07-20T10:13:15.03061Z","iopub.status.idle":"2023-07-20T10:13:30.663026Z","shell.execute_reply.started":"2023-07-20T10:13:15.030578Z","shell.execute_reply":"2023-07-20T10:13:30.661994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total_batches = len(data_generator)  # Get the total number of batches\n\n# for batch_index in range(total_batches):\n#     X_batch, y_batch = data_generator[batch_index]  # Retrieve a batch of data\n\n#     print(f\"Batch {batch_index+1}/{total_batches}\")\n#     print(f\"X_batch shape: {X_batch.shape}\")\n#     print(f\"y_batch shape: {y_batch.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:13:30.664945Z","iopub.execute_input":"2023-07-20T10:13:30.665378Z","iopub.status.idle":"2023-07-20T10:13:30.670475Z","shell.execute_reply.started":"2023-07-20T10:13:30.665337Z","shell.execute_reply":"2023-07-20T10:13:30.669478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n\n\n    # Perform the training step with the current batch\nhistory = model.fit(data_generator,\n                    epochs=100,\n                    validation_data=(val_images, val_masks),\n                    callbacks=[checkpoint, lr_scheduler])\n\n# Train the model\n\n# Evaluate the model\n","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:13:30.671761Z","iopub.execute_input":"2023-07-20T10:13:30.672601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss per epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dir = sys.path[0] + '/kaggle/input/hubmap-hacking-the-human-vasculature/test/'\n\nsubmission = pd.DataFrame()\nids = []\nh = []\nw = []\npred_strings = []\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_test=[]\nmasks_test=[]\npredictions = []\nfor each_image in os.listdir(test_dir):\n    image = cv2.imread(test_dir + each_image)\n    image = np.asarray(image, dtype=np.uint8)\n    height, width, channels = curr_img.shape\n    \n    predicted_mask = model.predict(np.expand_dims(image_gray, axis=0))\n    \n    h.append(height)\n    w.append(width)\n    \n    ## Get prediction_string\n    pred_strings.append(get_pred_string(objects))\n\n    \nsubmission[\"id\"] = ids\nsubmission[\"height\"] = h\nsubmission[\"width\"] = w\nsubmission[\"prediction_string\"] = pred_strings\nsubmission.set_index(\"id\", inplace=True)\nsubmission.to_csv(\"submission.csv\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}